{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6aca9246",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import mmap\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29856cac",
   "metadata": {},
   "source": [
    "# Split parallel text into syntax groups\n",
    "\n",
    "We split the parallel data into syntax groups depending on the first split in the syntax tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8abd5e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_lines(file_path):\n",
    "    fp = open(file_path, \"r+\")\n",
    "    buf = mmap.mmap(fp.fileno(), 0)\n",
    "    lines = 0\n",
    "    while buf.readline():\n",
    "        lines += 1\n",
    "    return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d91bcc9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_syntax_groups(cfg_file):\n",
    "    \"\"\"Gets count of top syntax groups from a parsed corpus\"\"\"\n",
    "    \n",
    "    prods = []\n",
    "    \n",
    "    with open(cfg_file, 'r') as f:\n",
    "        lines = [l.strip() for l in f.readlines()]\n",
    "    for line in tqdm(lines):\n",
    "        tree = nltk.Tree.fromstring(line)\n",
    "        prod = tree.productions()\n",
    "        first_split = prod[1]\n",
    "        prods.append(first_split)\n",
    "            \n",
    "    return prods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "648b7659",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_training_data_in_group(src_file, trg_file, mask0, mask1, mask2):\n",
    "    \"\"\"Gets training data for fine-tuning for three syntax groups\"\"\"\n",
    "    \n",
    "    # read in files\n",
    "    print(\"reading source file\")\n",
    "    with open (src_file, 'r') as f:\n",
    "        src = [line.strip() for line in f.readlines()]\n",
    "    print(\"reading target file\")\n",
    "    with open (trg_file, 'r') as f:\n",
    "        trg = [line.strip() for line in f.readlines()]\n",
    "        \n",
    "    # create training subsets for each mask\n",
    "    src0, src1, src2 = [], [], []\n",
    "    trg0, trg1, trg2 = [], [], []\n",
    "    \n",
    "    for i, line in enumerate(tqdm(src, \"splitting data\")):\n",
    "        if mask0[i]:\n",
    "            src0.append(line)\n",
    "            trg0.append(trg[i])\n",
    "        elif mask1[i]:\n",
    "            src1.append(line)\n",
    "            trg1.append(trg[i])\n",
    "        elif mask2[i]:\n",
    "            src2.append(line)\n",
    "            trg2.append(trg[i])\n",
    "            \n",
    "    return src0, trg0, src1, trg1, src2, trg2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02103edf",
   "metadata": {},
   "source": [
    "## Icelandic - English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b5990d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "CFG_FILE = '/rds/user/cs-burc1/hpc-work/datasets/parallel-data/is-en/parsed/train.en.combo.cfg'\n",
    "SRC_FILE = '/rds/user/cs-burc1/hpc-work/datasets/parallel-data/is-en/sp/train.sp.en'\n",
    "TRG_FILE = '/rds/user/cs-burc1/hpc-work/datasets/parallel-data/is-en/sp/train.sp.is'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "35b90f58",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████| 2965788/2965788 [09:18<00:00, 5309.46it/s]\n"
     ]
    }
   ],
   "source": [
    "group_labels = get_syntax_groups(CFG_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bae93d81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(S -> NP VP ., 912103),\n",
       " (NP -> NP PP, 107479),\n",
       " (S -> PP , NP VP ., 102788),\n",
       " (S -> VP ., 90083),\n",
       " (S -> SBAR , NP VP ., 76263),\n",
       " (S -> VP, 68679),\n",
       " (S -> S CC S ., 66483),\n",
       " (S -> S , CC S ., 65447),\n",
       " (S -> NP VP, 61464),\n",
       " (S -> ADVP NP VP ., 48073),\n",
       " (S -> NP VP :, 43766),\n",
       " (S -> NP ADVP VP ., 38455),\n",
       " (S -> ADVP , NP VP ., 38360),\n",
       " (S -> S , NP VP ., 36934),\n",
       " (S -> PP NP VP ., 31149),\n",
       " (S -> S : S ., 29777),\n",
       " (S -> SYM VP, 25527),\n",
       " (S -> CC NP VP ., 25002),\n",
       " (FRAG -> NP : NP, 24114),\n",
       " (S -> S . S ., 22590)]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# most common groups\n",
    "prod_count = Counter(group_labels)\n",
    "prod_count.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "689c4ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get top three most common groups\n",
    "group0 = str(prod_count.most_common()[0][0])\n",
    "group1 = str(prod_count.most_common()[1][0])\n",
    "group2 = str(prod_count.most_common()[2][0])\n",
    "\n",
    "# create masks for groups\n",
    "mask0 = [str(label) == group0 for label in group_labels]\n",
    "mask1 = [str(label) == group1 for label in group_labels]\n",
    "mask2 = [str(label) == group2 for label in group_labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7b4c7f08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading source file\n",
      "reading target file\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "splitting data: 100%|███████████████████████| 2965788/2965788 [00:02<00:00, 1370032.30it/s]\n"
     ]
    }
   ],
   "source": [
    "data = get_training_data_in_group(SRC_FILE, TRG_FILE, mask0, mask1, mask2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "8af6bf84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write out data\n",
    "OUTDIR = \"/home/cs-burc1/projects/diversity-bt/experiments/datasets/parallel-data/is-en/sp\"\n",
    "for i in range(3):\n",
    "    with open(f\"{OUTDIR}/syntaxdata{i}.sp.en\", \"w\") as f:\n",
    "        f.writelines([line + '\\n' for line in data[i*2]])\n",
    "    with open(f\"{OUTDIR}/syntaxdata{i}.sp.is\", \"w\") as f:\n",
    "        f.writelines([line + '\\n' for line in data[i*2+1]])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
